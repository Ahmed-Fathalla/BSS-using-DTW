{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82480151",
   "metadata": {},
   "source": [
    "# Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bf990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install similaritymeasures\n",
    "# !pip install scikit-learn-extra\n",
    "# !pip install geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5db5f",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f701d7b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Step_1: Download data From: https://www.kaggle.com/datasets/vineethakkinapalli/citibike-bike-sharingnewyork-cityjan-to-apr-2021\n",
    "# Step_2: Merge all csv files into one file\n",
    "# from glob import glob\n",
    "# combined_df = pd.DataFrame()\n",
    "# for file in sorted(glob('CitiBike_Trip_Data/*/*.csv')):\n",
    "#     df = pd.read_csv(file)\n",
    "#     print(df.shape)\n",
    "#     combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "# combined_df.to_csv('Citibike BikeSharing-NewYork-2020-2021-combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6efd05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:30:53.471813Z",
     "start_time": "2024-05-15T16:30:45.030728Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from Bike_utils_similarity.Exp import *\n",
    "from Bike_utils_similarity.extra_utils import data_partition,write_\n",
    "from Bike_utils_similarity.utils.pkl_utils import *\n",
    "from Bike_utils_similarity.utils.time_utils import get_TimeStamp_str\n",
    "from Bike_utils_similarity.utils.distance_extraction import get_distance\n",
    "from Bike_utils_similarity.similarity_utils import similarity, get_hours_curve_data\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79684588",
   "metadata": {},
   "source": [
    "# Renaming cols to be similar to \"2015-2016 dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "800945bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T15:47:05.928053Z",
     "start_time": "2024-05-08T15:47:05.744033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID',\n",
       "       'Start Station Name', 'Start Station Latitude',\n",
       "       'Start Station Longitude', 'End Station ID', 'End Station Name',\n",
       "       'End Station Latitude', 'End Station Longitude', 'Bike ID', 'User Type',\n",
       "       'Birth Year', 'Gender', 'Trip_Duration_in_min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_cols = ['tripduration', 'starttime', 'stoptime', 'start station id',\n",
    "                'start station name', 'start station latitude',\n",
    "                'start station longitude', 'end station id', 'end station name',\n",
    "                'end station latitude', 'end station longitude', 'bikeid', 'usertype',\n",
    "                'birth year', 'gender']\n",
    "new_cols = [ 'Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID', \n",
    "                'Start Station Name', 'Start Station Latitude',\n",
    "                'Start Station Longitude', 'End Station ID', 'End Station Name',\n",
    "                'End Station Latitude', 'End Station Longitude', 'Bike ID', 'User Type',\n",
    "                'Birth Year', 'Gender']\n",
    "cols_naming = {}\n",
    "for i,j in zip(current_cols,new_cols):cols_naming[i]=j\n",
    "\n",
    "df.rename(columns=cols_naming, inplace=True)\n",
    "\n",
    "df['Trip_Duration_in_min'] = df['Trip Duration']/60\n",
    "df['Trip_Duration_in_min'] = df['Trip_Duration_in_min'].astype(int)\n",
    "\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa956e",
   "metadata": {},
   "source": [
    "# Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========    Outlier Removal Start  =========================================================================================================================\n",
    "# --------------- 1 start \n",
    "reduction_method = '-reduced_Extra-60'\n",
    "df = df[df['Trip_Duration_in_min']<60]\n",
    "# ==========    Outlier Removal End  ========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76560221",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9092f09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T15:47:41.404987Z",
     "start_time": "2024-05-08T15:47:40.763105Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------- 1 start\n",
    "le = LabelEncoder()\n",
    "df['User Type'] = le.fit_transform(df['User Type'])  \n",
    "\n",
    "a = pd.to_datetime(df['Start Time'])\n",
    "df['DO_week'] = a.dt.day_of_week\n",
    "df['DO_month'] = a.dt.day\n",
    "df['month'] = a.dt.month\n",
    "df['hour'] = a.dt.hour\n",
    "df['min'] = a.dt.minute\n",
    "df['year'] = a.dt.year\n",
    "\n",
    "df['age'] = df['year'] - df['Birth Year']\n",
    "\n",
    "df['Start Hour'] = a.dt.hour \n",
    "# --------------- 1 end   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcbb516",
   "metadata": {},
   "source": [
    "## Encoding \"Start\" & \"End\" Station_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ccaf9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T15:48:19.649826Z",
     "start_time": "2024-05-08T15:48:13.930755Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fathalla\\AppData\\Local\\Temp\\ipykernel_27908\\1276063541.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  mappping_dic = dd.set_index('Start Station Name').T.to_dict('list')\n"
     ]
    }
   ],
   "source": [
    "# ========== mappping_dic ===============================\n",
    "# --------------- 2 start \n",
    "e = LabelEncoder()\n",
    "df['Start Station Encoding'] = e.fit_transform(df['Start Station Name'])    \n",
    "dd = df[['Start Station Name', 'Start Station Encoding']]\n",
    "mappping_dic = dd.set_index('Start Station Name').T.to_dict('list')\n",
    "# dump(mappping_dic,'mappping_dic')\n",
    "\n",
    "for k,v in mappping_dic.items():\n",
    "    mappping_dic[k] = v[0]\n",
    "    \n",
    "def custom_encoding(x):\n",
    "    return mappping_dic[x] if x in mappping_dic.keys() else -1\n",
    "\n",
    "df['End Station Encoding'] = df['End Station Name'].apply(lambda x:custom_encoding(x))\n",
    "df = df[~(df['End Station Encoding']==-1) ]\n",
    "dump(mappping_dic,'dump/mappping_dic-2020-2021'+reduction_method)\n",
    "# --------------- 2 End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e1f4095",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T15:49:35.094870Z",
     "start_time": "2024-05-08T15:48:32.223384Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------- 3 Start: Calc Distance\n",
    "from Bike_utils_similarity.utils.distance_extraction import get_distance\n",
    "df['distance'] = df[['Start Station Latitude',\n",
    "                     'Start Station Longitude',\n",
    "                     'End Station Latitude',\n",
    "                     'End Station Longitude']].apply(lambda x: get_distance(x), axis=1)\n",
    "# --------------- 3 End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0782a3",
   "metadata": {},
   "source": [
    "# Save Modified DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c167b4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T15:49:45.170227Z",
     "start_time": "2024-05-08T15:49:41.042284Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('Citibike BikeSharing-NewYork-2020-2021-modified%s.csv'%reduction_method, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1403a1",
   "metadata": {},
   "source": [
    "# Steps for the proposed method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b54b7223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T15:49:52.135581Z",
     "start_time": "2024-05-08T15:49:51.950617Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------- 4 start    \"dic.csv\"\n",
    "count = 0\n",
    "l=[]\n",
    "for i in range(len(mappping_dic)):\n",
    "    for j in range(i, len(mappping_dic)):\n",
    "        l.append(['%d-%d'%(i,j), count])\n",
    "        count+=1\n",
    "dic_ = pd.DataFrame( np.array(l), columns = ['key', 'value'])     \n",
    "dic_.to_csv('dump/dic-2020-2021%s.csv'%reduction_method, index=False)\n",
    "# --------------- 4 End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5badec79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T15:50:58.272744Z",
     "start_time": "2024-05-08T15:50:55.480074Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------- 5 start: Count number of trips between two stations\n",
    "data_partition_ = data_partition(df.copy())\n",
    "\n",
    "v_lst = list(mappping_dic.values())\n",
    "z = np.zeros((len(v_lst), len(v_lst)), dtype=int)\n",
    "mappping_dic = load_pkl('dump/mappping_dic-2020-2021') # =======================================\n",
    "\n",
    "for i in range(len(mappping_dic.values())):\n",
    "    for j in range(i, len(mappping_dic.values())):\n",
    "        z[i,j] = data_partition_.get_data( v_lst[i], v_lst[j], info_='shape')\n",
    "        \n",
    "d = pd.DataFrame(z, columns=v_lst)\n",
    "d.to_csv('dump/Trip count_martix-2020-2021%s.csv'%reduction_method, index=False)      \n",
    "# --------------- 5 End    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b2d3d27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T15:51:04.276402Z",
     "start_time": "2024-05-08T15:51:00.577765Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------- 6 start - Generate Curves data\n",
    "Conf_matrix_cell_curve_data = {}\n",
    "v_lst = list(mappping_dic.values())\n",
    "data_partition_ = data_partition(df.copy())\n",
    "\n",
    "for i in range(len(mappping_dic.values())):\n",
    "    for j in range(i, len(mappping_dic.values())):\n",
    "        dframe = data_partition_.get_data( v_lst[i], v_lst[j], info_='data')\n",
    "        Conf_matrix_cell_curve_data['%s-%s'%(i,j)] = get_hours_curve_data(dframe)\n",
    "        \n",
    "dump(Conf_matrix_cell_curve_data,'dump/Conf_matrix_cell_curve_data-2020-2021'+reduction_method)    \n",
    "# --------------- 6 End  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61490b85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T15:59:35.335842Z",
     "start_time": "2024-05-08T15:51:08.158327Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------- 7 start - \"dist_matrix\"\n",
    "Conf_matrix_cell_curve_data = load_pkl('dump/Conf_matrix_cell_curve_data-2020-2021'+reduction_method)\n",
    "k_lst = list(Conf_matrix_cell_curve_data.keys())\n",
    "dist_matrix = np.zeros((len(k_lst),len(k_lst)))\n",
    "\n",
    "similarity_ = similarity(Conf_matrix_cell_curve_data)\n",
    "\n",
    "for i in range(len(k_lst)):\n",
    "    for j in range(i+1, len(k_lst)):\n",
    "        x = similarity_.calc_similarity(k_lst[i], k_lst[j]) \n",
    "        dist_matrix[i,j] = x\n",
    "        dist_matrix[j,i] = x\n",
    "dist_matrix = pd.DataFrame( np.array(dist_matrix) )\n",
    "dist_matrix.to_csv('dump/dist_matrix-2020-2021%s.csv'%reduction_method, index=False)\n",
    "# --------------- 7 End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- 8 \"clustering.pkl\"  KMedoids:\n",
    "dist_matrix = pd.read_csv('dump/dist_matrix-2020-2021%s.csv'%reduction_method,index_col=None)\n",
    "res={}\n",
    "\n",
    "for i in range(2,11):\n",
    "    clustering_algo = KMedoids(n_clusters=i, random_state=0)\n",
    "    model = clustering_algo.fit(dist_matrix)\n",
    "    res[str(i)]=model.labels_\n",
    "\n",
    "dump(res, 'dump/clustering-%s-2020-2021%s'%(str(clustering_algo).split('(')[0], reduction_method)) #    of length 1326\n",
    "print('dump/clustering-%s-2020-2021%s'%(str(clustering_algo).split('(')[0], reduction_method))\n",
    "# --------------- 8 End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95b111",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2d219",
   "metadata": {},
   "source": [
    "## Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e7936ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T16:13:17.434477Z",
     "start_time": "2024-05-08T16:02:29.050609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcluster_df.shape ---------- cluster_id:0 (1092, 3)   DF length: 298786 \n",
      "\tcluster_df.shape ---------- cluster_id:1 (286, 3)   DF length: 94324 \n"
     ]
    }
   ],
   "source": [
    "clustering_method = 'KMedoids'\n",
    "\n",
    "df = pd.read_csv('Citibike BikeSharing-NewYork-2020-2021-modified%s.csv'%reduction_method)\n",
    "dic_df = pd.read_csv('dump/dic-2020-2021%s.csv'%reduction_method)\n",
    "res = load_pkl('dump/clustering-'+clustering_method+'-2020-2021'+reduction_method)\n",
    "data_partition_ = data_partition(df.copy())\n",
    "cluster_df = None\n",
    "\n",
    "for i in range(2,11):\n",
    "    exp_df = None\n",
    "    results = []\n",
    "    tmp = dic_df.copy()\n",
    "    tmp['cluster'] = res[str(i)]\n",
    "    cluster_id_results = []\n",
    "    for cluster_id in set(res[str(i)]):\n",
    "        cluster_df = tmp[tmp['cluster']==cluster_id]\n",
    "        cluster_id_df = pd.DataFrame()\n",
    "        for k in cluster_df['key'].values: #\n",
    "            w = [int(w) for w in k.split('-')]\n",
    "            sub_df = data_partition_.get_data(w[0], w[1], info_='data')\n",
    "            cluster_id_df = pd.concat([cluster_id_df, sub_df])\n",
    "        # train and predict the data ===========================\n",
    "        write_('output','\\tcluster_df.shape ---------- cluster_id:%d'%cluster_id , \n",
    "                                                                      cluster_df.shape, \n",
    "                                                                      \"  DF length:\", \n",
    "                                                                      cluster_id_df.shape[0] )\n",
    "        exp_res = ML_exp(cluster_id_df,\n",
    "                         start_model=0,\n",
    "                         end_model=10, # ==========================\n",
    "                         cluster='C-%s:%d-k:%d'%(clustering_method[:6],i,cluster_id),\n",
    "                         target=-1)\n",
    "        # get the results =======================================\n",
    "        cluster_id_results.extend(exp_res)\n",
    "\n",
    "    all_res = pd.DataFrame(np.array(cluster_id_results), columns=results_cols)\n",
    "    all_res.to_csv('res/cluster-%s-2020-2021 %d - %s%s.csv'%(clustering_method, i,\n",
    "                                                            get_TimeStamp_str(), \n",
    "                                                            reduction_method), \n",
    "                                                            index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af49e95",
   "metadata": {},
   "source": [
    "# ALL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f387cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Citibike BikeSharing-NewYork-2020-2021-modified%s.csv'%reduction_method)\n",
    "exp_res = ML_exp(df,\n",
    "                 start_model=0,\n",
    "                 end_model=10,\n",
    "                 cluster='all_data',\n",
    "                 target=-1)\n",
    "all_res = pd.DataFrame(np.array(exp_res), columns=results_cols)\n",
    "\n",
    "main_df = 'res/cluster-2020-2021 all_data - %s%s.csv'%(get_TimeStamp_str(),\n",
    "                                                            reduction_method)\n",
    "all_res.to_csv(main_df, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fae78e",
   "metadata": {},
   "source": [
    "# Summarize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bike_utils_similarity.extra_utils import *\n",
    "from Bike_utils_similarity.utils.time_utils import get_TimeStamp_str\n",
    "\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "from glob import glob\n",
    "\n",
    "# clustering_method = ''\n",
    "files = sorted(glob('res/cluster-%s*.csv'%clustering_method)) \n",
    "\n",
    "rs = ['Trip_Duration_in_min', 'distance']\n",
    "ms = ['RandomFore', 'BaggingReg', '<catboost.']\n",
    "\n",
    "res_df = None\n",
    "main_df = pd.read_csv(main_df)\n",
    "for f in files:\n",
    "    d_file = pd.read_csv(f)\n",
    "    for r in rs:\n",
    "        for m in ms:\n",
    "            d = main_df.copy()\n",
    "            d = d[( (d['model']==m) & (d['response']==r)) ]\n",
    "            df = d_file.copy()\n",
    "            df = df[( (df['model']==m) & (df['response']==r)) ]\n",
    "            a = df.describe()\n",
    "            a = a.loc[['mean'], :]  # 'mean', 'std'\n",
    "\n",
    "            res_df = pd.concat([res_df, d,empty_row(d.columns), df,a, empty_row(d.columns, 4)])\n",
    "\n",
    "res_df.to_excel('summary-%s-%s-%s.xlsx'%(clustering_method, get_TimeStamp_str(), reduction_method), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
